---
title: "savingSessions: tweet analysis"
author: "Ben Anderson (@dataknut)"
date: 'Last run at: `r Sys.time()`'
format: 
  html:
    self-contained: true
    toc: true
    toc-depth: 3
    code-fold: true
    number-sections: true
execute:
  echo: false
  warning: false
editor: visual
---

# Background

UK demand response experiments by NG-ESO and retailers such as [\@OctopusEnergy](https://twitter.com/SavingSessions)

Attempt to do some analysis of #savingSession(s) tweets.

Inspired by [https://docs.ropensci.org/rtweet/](https://docs.ropensci.org/rtweet/)

Last run at: `r Sys.time()`

# Code setup

Part of <https://github.com/dataknut/savingSessions>

Makes use of <https://github.com/dataknut/hashTagR>, a DIY wrapper for the [rtweet](https://docs.ropensci.org/rtweet) rstats package.

```{r}
#| label: codeSetup
#| warning: false

myLibs <- c("data.table",
            "hashTagR",
            "ggplot2",
            "lubridate",
            "readr",
            "rtweet",
            "tidyverse",
            "tidytext",
            "wordcloud")

# load the libraries, install if we don't have them
dkUtils::loadLibraries(myLibs) # get it here: devtools::install_github("dataknut/dkUtils")

hashtags <- c("savingSession", "savingSessions", "savingsession") # fairly sure it's case insensitive
searchString <- hashTagR::createSearchFromTags(hashtags) # convert to rtweet search string

#searchString <- "#SavingSession OR #SavingSessions" # edit to suit

dataPath <- "~/Dropbox/data/twitter/savingSessions/" # edit to suit

session1DateStart <- lubridate::as_datetime("2022-11-15 17:00:00") # the half-hour it starts
session1DateEnd <- lubridate::as_datetime("2022-11-15 18:00:00") # 

session2DateStart <- lubridate::as_datetime("2022-11-22 17:30:00") # the half-hour it starts
session2DateEnd <- lubridate::as_datetime("2022-11-22 18:30:00") # 


session3DateStart <- lubridate::as_datetime("2022-11-30 17:30:00") # the half-hour it starts
session3DateEnd <- lubridate::as_datetime("2022-11-30 18:30:00") # 


session4DateStart <- lubridate::as_datetime("2022-12-01 17:00:00") # the half-hour it starts
session4DateEnd <- lubridate::as_datetime("2022-12-01 18:00:00") # 

session5DateStart <- lubridate::as_datetime("2022-12-12 17:00:00") # the half-hour it starts
session5DateEnd <- lubridate::as_datetime("2022-12-12 18:30:00") # 

# functions ----

add_sessionDate <- function(p){
  # add session dates to plot
  p <- p +
  annotate("rect", xmin = session1DateStart,
           xmax = session1DateEnd,
           ymin = ymin, ymax = ymax,
           alpha = periodAlpha, fill = periodFill) +
    annotate("text", x = session1DateStart-(24*60*60), 
             y = ymax*0.9, label = paste0("Session 1\n",
                                          lubridate::as_date(session1DateStart))) + 
  annotate("rect", xmin = session2DateStart,
           xmax = session2DateEnd,
           ymin = ymin, ymax = ymax,
           alpha = periodAlpha, fill = periodFill) +
        annotate("text", x = session2DateStart-(24*60*60), 
             y = ymax*0.9, label = paste0("Session 2\n",
                                          lubridate::as_date(session2DateStart))) + 
  annotate("rect", xmin = session3DateStart,
           xmax = session3DateEnd,
           ymin = ymin, ymax = ymax,
           alpha = periodAlpha, fill = periodFill) +
        annotate("text", x = session3DateStart-(24*60*60), 
             y = ymax*0.9, label = paste0("Session 3\n",
                                          lubridate::as_date(session3DateStart)))  +
    annotate("rect", xmin = session4DateStart,
           xmax = session4DateEnd,
           ymin = ymin, ymax = ymax,
           alpha = periodAlpha, fill = periodFill) +
        annotate("text", x = session4DateStart-(24*60*60), 
             y = ymax*0.75, label = paste0("Session 4\n",
                                          lubridate::as_date(session4DateStart))) +
      annotate("rect", xmin = session5DateStart,
           xmax = session5DateEnd,
           ymin = ymin, ymax = ymax,
           alpha = periodAlpha, fill = periodFill) +
        annotate("text", x = session5DateStart-(24*60*60), 
             y = ymax*0.9, label = paste0("Session 5\n",
                                          lubridate::as_date(session5DateStart)))
  return(p)
}
```

# Getting data

Grab the most recent set of tweets that mention `r searchString` using the [rtweet::search_tweet()](https://docs.ropensci.org/rtweet/reference/search_tweets.html) function and merge with any we may already have downloaded.

> Should we also try to get all replies to @savingSessions?

Note that tweets do not seem to be available after \~ 14 days via the API used by rtweet. Best to keep refreshing the data every week...

```{r}
#| label: getData

now <- lubridate::now()

if(dir.exists(dataPath)){
  ofile <- path.expand(paste0(dataPath, "tw_",now,".csv"))
  tweetsDT <- hashTagR::getTweets(searchString, n = 20000)
  readr::write_csv(tweetsDT, file = ofile) # data.table breaks here
  message("Retrieved ", nrow(tweetsDT), " tweets and saved them to ", ofile)
} else {
  message(dataPath, " not found!")
}

# now load all the tweet files we have matching that searchString

tweetsDT <- hashTagR::loadTweets(path = dataPath, pattern = "*.csv")

uniq_tweetsDT <- hashTagR::processTweets(tweetsDT)
```

That produced a data file of `r nrow(uniq_tweetsDT)` tweets.

We do NOT store the tweets in the repo for both [ethical](https://blogs.lse.ac.uk/impactofsocialsciences/2015/09/28/challenges-of-using-twitter-as-a-data-source-resources/) and practical reasons...

Note also that we may not be collecting the complete dataset of hashtagged tweets due to the [intricacies of the twitter API](https://www.demos.co.uk/files/Road_to_representivity_final.pdf?1441811336).

# Analysis

## Tweet time line

@fig-timeLine shows the timing of tweets by hour.

```{r}
#| echo: false
#| label: fig-timeLine
#| fig-cap: "Tweets over time"

plotDT <- uniq_tweetsDT[, .(nObs = .N), keyby = .(Time = ba_created_at_dateHour)]

periodAlpha <- 0.3 #  shaded rects on plots
periodFill <- "grey50"
ymax <- max(plotDT$nObs)
ymin <- min(plotDT$nObs)


# plot over time

myCaption <- "Data: twitter API via @mkearney's rtweet package\nPlot by @dataknut\nSaving Session period(s) marked as grey box"
p <- ggplot2::ggplot(plotDT, aes(x = Time, y = nObs)) + 
  geom_point() +
  labs(caption = myCaption)
add_sessionDate(p)
```

@fig-cumulativeTimeLine shows cumulative tweets by hour.

```{r}
#| echo: false
#| label: fig-cumulativeTimeLine
#| fig-cap: "Cumulative number of tweets over time"


plotDT[, cumSum := cumsum(nObs)]
ymax <- max(plotDT$cumSum)
ymin <- min(plotDT$cumSum)
p <- ggplot2::ggplot(plotDT, aes(x = Time, y = cumSum)) + 
  geom_point() +
  geom_line() +
  labs(caption = myCaption)

add_sessionDate(p)
```

We see roughly the kind of uptick in tweets for Session 2 that we saw for Session 1...

## Content analysis

Let's try a word cloud.

Inspiration here: <https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a>

```{r}
#| label: cleanTweets

uniq_tweetsDT[, clean_text := gsub("https\\S*", " ", text)]
uniq_tweetsDT[, clean_text := gsub("@\\S*", " ", clean_text)]
uniq_tweetsDT[, clean_text := gsub("amp", " ", clean_text)]
uniq_tweetsDT[, clean_text := gsub("RT", " ", clean_text)]
uniq_tweetsDT[, clean_text := gsub("[\r\n]", " ", clean_text)]
uniq_tweetsDT[, clean_text := gsub("[[:punct:]]", " ", clean_text)]

```

```{r}
#| label: makeWords

tweets_words <-  uniq_tweetsDT %>%
 select(clean_text) %>%
 unnest_tokens(word, clean_text)

words <- tweets_words %>% count(word, sort=TRUE)
```

Make a word cloud for all tweets

These are unlikely to render the word 'savingsession' as it will be in all tweets due to the twitter search pattern used.

```{r}
#| label: makeCloud

set.seed(1234) # for reproducibility 
wordcloud::wordcloud(words = words$word, 
          freq = words$n, min.freq = 1,           
          max.words=200, 
          random.order=FALSE, 
          rot.per=0.35,            
          colors=brewer.pal(8, "Dark2"))
```

We need to remove common words (to, the, and, a, for, etc). These are called 'stop words'.

```{r}
#| label: cleanWordCloud

data(stop_words)

# remove the stop words - they are usually the most frequent
# and usually the least interesting
reduced_df <- words %>%
  dplyr::anti_join(stop_words)

# redraw
set.seed(1234) # for reproducibility 
wordcloud::wordcloud(words = reduced_df$word, 
                     freq = reduced_df$n, min.freq = 1,           
                     max.words=200, 
                     random.order=FALSE, 
                     rot.per=0.35,            
                     colors=brewer.pal(8, "Dark2"))
```

Not especially informative...

# Sentiment analysis

Inspired by <https://www.tidytextmining.com/sentiment.html>

Take those cleaned words and sentiment them!

The first word cloud are names that have negative sentiment (according to `tidytext::get_sentiments("bing")`). Remember the size of the words is relative to the count of other negative words.

```{r}
#| label: sentiment
bing_sentiment_counts <- reduced_df %>%
  inner_join(tidytext::get_sentiments("bing")) %>% # only keeps those with sentiments
  #count(linenumber, sentiment, sort = TRUE) %>%
  ungroup()

message("\nDo we have roughly the same number of each kind of word?")
table(bing_sentiment_counts$sentiment)

message("What about the overall frequency of these words?")
bing_sentiment_counts %>%
  group_by(sentiment) %>%
  summarize(freq=sum(n))

```

```{r}
#| label: sentimentNeg
#| 
neg <- bing_sentiment_counts %>%
  filter(sentiment == "negative")

  wordcloud::wordcloud(words = neg$word, 
                     freq = neg$n, min.freq = 1,           
                     max.words=200, 
                     random.order=FALSE, 
                     rot.per=0.35,            
                     colors=brewer.pal(8, "Dark2"))
```

The second wordcloud shows positive sentiments for all tweets. Remember the size of the words is relative to the count of other positive words.

```{r}
#| label: sentimentPos
bing_sentiment_counts <- reduced_df %>%
  inner_join(tidytext::get_sentiments("bing")) %>% # only keeps those with sentiments
  #count(linenumber, sentiment, sort = TRUE) %>%
  ungroup()

pos <- bing_sentiment_counts %>%
  filter(sentiment == "positive")

  wordcloud::wordcloud(words = pos$word, 
                     freq = pos$n, min.freq = 1,           
                     max.words=200, 
                     random.order=FALSE, 
                     rot.per=0.35,            
                     colors=brewer.pal(8, "Dark2"))
```

## Session 1 sentiment word clouds

Repeat these word clouds but just for the first session which was on `r as.Date(session1DateStart)`.

These are just the tweets for the day of the event and the day after...

```{r}
#| label: session1SentimentCloud

makeSentimentCloud <- function(dt){
  # start with the selected tweets
  # grab the words
  tweets_words <-  dt %>%
    select(clean_text) %>%
    unnest_tokens(word, clean_text)
  # count the words
  words <- tweets_words %>% count(word, sort=TRUE)
  # remove the stop words
  data(stop_words)
  
  # remove the stop words - they are usually the most frequent
  # and usually the least interesting
  de_stopped <- words %>%
    dplyr::anti_join(stop_words)
  
  bsc <- de_stopped %>%
    inner_join(tidytext::get_sentiments("bing")) %>% # only keeps those with sentiments
    #count(linenumber, sentiment, sort = TRUE) %>%
    ungroup()
  
    wc <- list()
    
  # select the sentiment to wordCloud
  wc_df <- bsc %>%
    filter(sentiment == "positive")
  
  set.seed(1234) # for reproducibility
  wc$plot_pos <- wordcloud::wordcloud(words = wc_df$word, 
                             freq = wc_df$n, min.freq = 1,           
                             max.words=200, 
                             random.order=FALSE, 
                             rot.per=0.35,            
                             colors=brewer.pal(8, "Dark2"))
  wc_df <- bsc %>%
    filter(sentiment == "negative")
  
  wc$plot_neg <- wordcloud::wordcloud(words = wc_df$word, 
                             freq = wc_df$n, min.freq = 1,           
                             max.words=200, 
                             random.order=FALSE, 
                             rot.per=0.35,            
                             colors=brewer.pal(8, "Dark2"))
  return(wc)
}

session1Tweets <- uniq_tweetsDT[ba_created_at_date >= as.Date(session1DateStart) &
                                 ba_created_at_date <= as.Date(session1DateEnd)+1]


wc <- makeSentimentCloud(session1Tweets)

wc$plot_neg
wc$plot_pos

```

## Session 2 word clouds

Repeat for session 2 which was on `r as.Date(session2DateStart)`.

These are just the tweets for the day of the event and the day after...

```{r}
#|label: session2SentimentCloud
session2Tweets <- uniq_tweetsDT[ba_created_at_date >= as.Date(session2DateStart) &
                                 ba_created_at_date <= as.Date(session2DateEnd)+1]


wc <- makeSentimentCloud(session2Tweets)

wc$plot_neg
wc$plot_pos
```

## Session 3 word clouds

Repeat for session 3 which which was on `r as.Date(session3DateStart)`.

These are just the tweets for the day of the event and the day after...

```{r}
#|label: session3SentimentCloud

session3Tweets <- uniq_tweetsDT[ba_created_at_date >= as.Date(session3DateStart) &
                                 ba_created_at_date <= as.Date(session3DateEnd)+1]


wc <- makeSentimentCloud(session3Tweets)

wc$plot_neg
wc$plot_pos

```

## Session 4 word clouds

Repeat for session 4 which was on `r as.Date(session4DateStart)`.

These are just the tweets for the day of the event and the day after...

```{r}
#|label: session4SentimentCloud

session4Tweets <- uniq_tweetsDT[ba_created_at_date >= as.Date(session4DateStart) &
                                 ba_created_at_date <= as.Date(session4DateEnd)+1]

wc <- makeSentimentCloud(session4Tweets)

wc$plot_neg
wc$plot_pos
```

## Session 5 word clouds

Repeat for session 5 which was on `r as.Date(session5DateStart)`.

These are just the tweets for the day of the event and the day after...

```{r}
#|label: session5SentimentCloud

session5Tweets <- uniq_tweetsDT[ba_created_at_date >= as.Date(session5DateStart) &
                                 ba_created_at_date <= as.Date(session5DateEnd)+1]

wc <- makeSentimentCloud(session5Tweets)

wc$plot_neg
wc$plot_pos
```

# The end

```{r}
#| label: theEnd
```
